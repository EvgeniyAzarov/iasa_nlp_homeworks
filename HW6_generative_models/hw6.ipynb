{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Read about difference between GPT-3.5 and GPT-4.\n",
    "\n",
    "Read about metrics for generarive NLP.\n",
    "\n",
    "**Advanced**: Generative models are usually very big. Read about model quantization. That may help with inference of big models such as GPT.\n",
    "\n",
    "**Theory** (5 points): Google form questions.\n",
    "\n",
    "**Practical task** (10 points): \n",
    "1. Choose one:\n",
    "    * Finetune transformer model for summarization on https://huggingface.co/datasets/samsum.\n",
    "    * Finetune transformer model for translation on dataset of your choice.\n",
    "2. Experiment with different prompts.\n",
    "2. Based on a task you choose, choose a few metrics that are used in generative NLP (BLEU, ROUGE etc), test your finetune models using them, describe their pros and cons relative to the generations your model makes.\n",
    "\n",
    "3. If you want, you can try use LoRA or prefix tuning for finetuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.20.6)\n",
      "Requirement already satisfied: evaluate in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: rouge_score in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: texttable in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (3.19.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: psutil in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (5.9.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.17.3)\n",
      "Requirement already satisfied: packaging in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: click in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install py7zr evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, prefix):\n",
    "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 model was pre-trained on different tasks, including summarization. During the pre-training stage the prefix `\"summarize: \"` was used for summarization task, and thus it should be used for summarization inferences. We will try to investigate, what whould happen if we replace this prompt with more precise `\"summarize the following dialogue: \"` during the fine-tuning on the dataset of dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b94dd420c24a9bb660a6dd7149fa70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea238e8a558d43899ee1c33460fc3972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285686991e154ba0ab6bbc84668adcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfe70b56b3f4f4d844dea220cdd5a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad8e79702294c4495d064ea43ec70cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d352d9c63a51424d8eca99e93a815cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_samsum = samsum.map(preprocess_function, batched=True, fn_kwargs={\"prefix\": \"summarize: \"})\n",
    "tokenized_samsum_new_prompt = samsum.map(preprocess_function, batched=True, fn_kwargs={\"prefix\": \"summarize the following dialogue: \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/t5-samsum\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=6,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum[\"train\"],\n",
    "    eval_dataset=tokenized_samsum[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "my_secret = user_secrets.get_secret(\"wandb_api\") \n",
    "\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myevhenii-azarov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231106_022236-ucffq3jv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mleafy-tree-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface/runs/ucffq3jv\u001b[0m\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5526' max='5526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5526/5526 29:47, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.257200</td>\n",
       "      <td>1.873949</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>16.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.034600</td>\n",
       "      <td>1.826320</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>16.509800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.985800</td>\n",
       "      <td>1.803390</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>16.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.958500</td>\n",
       "      <td>1.786948</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>16.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.945600</td>\n",
       "      <td>1.782817</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>16.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.931000</td>\n",
       "      <td>1.780898</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>16.636900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len ▁▅▄▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 ▁▄▆█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 ▁▄▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL ▁▄▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum ▁▄▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▂▂▃▃▄▄▄▅▅▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▂▂▃▃▄▄▄▅▅▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▇▆▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▅▃▃▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len 16.6369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.7809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 0.4158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 0.1858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL 0.3492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum 0.3491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 22.1126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 6.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 5526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 9421008516022272.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.00679\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1820.4972\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 48.554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 3.035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mleafy-tree-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface/runs/ucffq3jv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231106_022236-ucffq3jv/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir = \"/kaggle/working/t5-samsum-newprompt\"\n",
    "trainer_new_prompt = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum_new_prompt[\"train\"],\n",
    "    eval_dataset=tokenized_samsum_new_prompt[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20231106_025303-lge5xhsq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/kaggle/working/t5-samsum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface/runs/lge5xhsq\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5526' max='5526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5526/5526 30:00, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.921200</td>\n",
       "      <td>1.761207</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>16.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.885600</td>\n",
       "      <td>1.746259</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>16.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.870700</td>\n",
       "      <td>1.739120</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>16.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.858900</td>\n",
       "      <td>1.730590</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>16.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.849500</td>\n",
       "      <td>1.728655</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>16.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.841300</td>\n",
       "      <td>1.728808</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>16.701700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len ▆█▁▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 ▁▂▃▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 ▁▃▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL ▁▃▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum ▁▃▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▆▃▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁▃▆█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁▃▆█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▂▂▃▃▄▄▄▅▅▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▂▂▃▃▄▄▄▅▅▆▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▇▆▅▅▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ██▅▅▄▃▃▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/gen_len 16.7017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.72881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge1 0.4289\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rouge2 0.2007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/rougeL 0.3634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 eval/rougeLsum 0.3634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 22.2472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 36.769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 6.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 5526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.8413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 9490952736079872.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.8708\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1831.8557\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 48.253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 3.017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m/kaggle/working/t5-samsum\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yevhenii-azarov/huggingface/runs/lge5xhsq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231106_025303-lge5xhsq/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer_new_prompt.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant signs of overfitting, we will use the last snapshots for both cases for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE score (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly used for text summarization tasks. ROUGE was designed to evaluate the quality of machine-generated summaries by comparing them to reference summaries provided by humans. Here we will compare several different variants of the ROUGE scores.\n",
    "\n",
    "**ROUGE-N**\n",
    "\n",
    "Measures overlap of n-grams (contiguous sequences of n words) between the reference and generated summaries.\n",
    "- ROUGE-1: variant with unigrams\n",
    "- ROUGE-2: variant with bigrams\n",
    "\n",
    "ROUGE-N is sensitive to phrase matching. It is simple and easy to compute. Among cons, it does not explicitly account for word order, which may limit its ability to capture the coherence and fluency of summaries. Also, it may penalize synonyms or paraphrased expressions that convey similar meanings but use different words.\n",
    "\n",
    "**ROUGE-L**\n",
    "\n",
    "Computes the longest common subsequence between reference and generated summaries, considering word sequences.\n",
    "- ROUGE-L: computes average recall based among sentences (splits text by '.')\n",
    "- ROUGE-Lsum: computes average recall among lines (splits text by '\\n') \n",
    "\n",
    "Longest common substring consideration can be beneficial for evaluating the coherence and structure of summaries. ROUGE-L is less sensitive to variations in word choice and allows for partial matches, making it more forgiving in certain cases. From the drawbacks perspective, the longest common subsequence approach might not always reflect the semantic similarity, especially in cases where there are multiple ways to form a valid subsequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py7zr evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966826f8d6eb46dca7bbdb4b0b738d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b94ff70fdd49218e993b8b551c6f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab7a3a7a2094e79a908ddecfd7bbaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba870a5c05c046afa52cd672491b26a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "samsum_test = load_dataset('samsum', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e885a45427e4862a3a779cd5d12bb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_prefix(examples):\n",
    "    examples['dialogue_prompt'] = [\"summarize: \" + doc for doc in examples['dialogue']]\n",
    "    examples['dialogue_prompt_new'] = [\"summarize the following dialogue: \" + doc for doc in examples['dialogue']]\n",
    "    return examples\n",
    "\n",
    "samsum_test = samsum_test.map(add_prefix, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_base = 't5-small'\n",
    "checkpoint_1 = '/kaggle/input/t5-samsum-tuning/t5-samsum/checkpoint-5526'\n",
    "checkpoint_2 = '/kaggle/input/t5-samsum-tuning/t5-samsum-newprompt/checkpoint-5526'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_base)\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_base)\n",
    "model_1 = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_1)\n",
    "model_2 = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_base = pipeline(\"summarization\", model_base, tokenizer=tokenizer, max_length=300, device=0)\n",
    "pipeline_1 = pipeline(\"summarization\", model_1, tokenizer=tokenizer, max_length=300, device=0)\n",
    "pipeline_2 = pipeline(\"summarization\", model_2, tokenizer=tokenizer, max_length=300, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN 🙊 Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting 😱 To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on 😂\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: 🙉\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan 😂\n",
      "Hilary: 😎\n",
      "Elliot: See you at 2 then xx\n"
     ]
    }
   ],
   "source": [
    "print(samsum_test['dialogue'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're meeting for the drinks in the evening anyway and we'll be going back to the apartment together? Hilary: I'm sooo tired after yesterday .\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_base(samsum_test['dialogue_prompt'][5])[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamin and Elliot will meet at lunchtime and take the keys and take a nap. They'll meet at the entrance to the conference hall at 2 pm and then go to La Cantina.\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_1(samsum_test['dialogue_prompt'][5])[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hilary and Elliot will meet at lunchtime, take the keys and take a nap. They'll meet at La Cantina at 2 pm.\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_2(samsum_test['dialogue_prompt_new'][5])[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_base = []\n",
    "for out in tqdm(pipeline_base(KeyDataset(samsum_test, \"dialogue_prompt\"))):\n",
    "    summaries_base.append(out[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_1 = []\n",
    "for out in tqdm(pipeline_1(KeyDataset(samsum_test, \"dialogue_prompt\"))):\n",
    "    summaries_1.append(out[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_2 = []\n",
    "for out in tqdm(pipeline_2(KeyDataset(samsum_test, \"dialogue_prompt_new\"))):\n",
    "    summaries_2.append(out[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_all = summaries_base\n",
    "summaries_1 = summaries_base[819:819*2]\n",
    "summaries_2 = summaries_base[819*2:]\n",
    "summaries_base = summaries_base[:819]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base</td>\n",
       "      <td>0.291530</td>\n",
       "      <td>0.084075</td>\n",
       "      <td>0.217757</td>\n",
       "      <td>0.217957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fine-tuned</td>\n",
       "      <td>0.423714</td>\n",
       "      <td>0.184818</td>\n",
       "      <td>0.327715</td>\n",
       "      <td>0.327795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fine-tuned new prompt</td>\n",
       "      <td>0.432573</td>\n",
       "      <td>0.193276</td>\n",
       "      <td>0.336552</td>\n",
       "      <td>0.336778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model    rouge1    rouge2    rougeL  rougeLsum\n",
       "0                   base  0.291530  0.084075  0.217757   0.217957\n",
       "1             fine-tuned  0.423714  0.184818  0.327715   0.327795\n",
       "2  fine-tuned new prompt  0.432573  0.193276  0.336552   0.336778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = []\n",
    "models = ['base', 'fine-tuned', 'fine-tuned new prompt']\n",
    "preds = [summaries_base, summaries_1, summaries_2]\n",
    "for model, predictions in zip(models, preds):\n",
    "    row = {\"model\": model}\n",
    "    metrics = rouge.compute(predictions=predictions, references=samsum_test['summary'], use_stemmer=True)\n",
    "    row.update(metrics)\n",
    "    res.append(row)\n",
    "res = pd.DataFrame.from_records(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although adding custom prompt during the fine-tuning breaks consitency with the pre-training stage, fine-tuned model with custom prompt showed better performance in all considered rouge metrics on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum_test = samsum_test.add_column('summaries_base', summaries_base)\n",
    "samsum_test = samsum_test.add_column('summaries_tuned', summaries_1)\n",
    "samsum_test = samsum_test.add_column('summaries_tuned_new_prompt', summaries_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum_test.to_csv(\"/kaggle/working/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
