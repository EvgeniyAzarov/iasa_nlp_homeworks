{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Read about difference between GPT-3.5 and GPT-4.\n",
    "\n",
    "Read about metrics for generarive NLP.\n",
    "\n",
    "**Advanced**: Generative models are usually very big. Read about model quantization. That may help with inference of big models such as GPT.\n",
    "\n",
    "**Theory** (5 points): Google form questions.\n",
    "\n",
    "**Practical task** (10 points): \n",
    "1. Choose one:\n",
    "    * Finetune transformer model for summarization on https://huggingface.co/datasets/samsum.\n",
    "    * Finetune transformer model for translation on dataset of your choice.\n",
    "2. Experiment with different prompts.\n",
    "2. Based on a task you choose, choose a few metrics that are used in generative NLP (BLEU, ROUGE etc), test your finetune models using them, describe their pros and cons relative to the generations your model makes.\n",
    "\n",
    "3. If you want, you can try use LoRA or prefix tuning for finetuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py7zr in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.20.6)\n",
      "Requirement already satisfied: evaluate in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: rouge_score in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: texttable in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (3.19.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: psutil in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from py7zr) (5.9.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.17.3)\n",
      "Requirement already satisfied: packaging in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: click in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/evgeniy/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install py7zr evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, prefix):\n",
    "    inputs = [prefix + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 model was pre-trained on different tasks, including summarization. During the pre-training stage the prefix `\"summarize: \"` was used for summarization task, and thus it should be used for summarization inferences. We will try to investigate, what whould happen if we replace this prompt with more precise `\"summarize the following dialogue: \"` during the fine-tuning on the dataset of dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8570646a58b543bfb449ecd2a18c6809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38681efeda84acd936a06d434c84863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a3c0ac9ec44d6aa8f1f14b1f9ca7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3323ecf016b947e6a8b8890f86740f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7cca0b85cb4796af575ff1a18a575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e74db6ce5d4a81b49f7491dc84f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_samsum = samsum.map(preprocess_function, batched=True, fn_kwargs={\"prefix\": \"summarize: \"})\n",
    "tokenized_samsum_new_prompt = samsum.map(preprocess_function, batched=True, fn_kwargs={\"prefix\": \"summarize the following dialogue: \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_awesome_billsum_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     save_total_limit\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     predict_with_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     push_to_hub\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/evgeniy/programming/study/NLP_IASA/iasa_nlp_homeworks/HW6_generative_models/hw6.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n",
      "File \u001b[0;32m<string>:122\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, neftune_noise_alpha, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n",
      "File \u001b[0;32m~/.local/bin/miniconda3/envs/nlp-env/lib/python3.11/site-packages/transformers/training_args.py:1448\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1439\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1447\u001b[0m ):\n\u001b[0;32m-> 1448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1450\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1451\u001b[0m     )\n\u001b[1;32m   1453\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1454\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1455\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1463\u001b[0m ):\n\u001b[1;32m   1464\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1465\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1466\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1467\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-samsum\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=6,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum[\"train\"],\n",
    "    eval_dataset=tokenized_samsum[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "training_args.output_dir = \"t5-samsum-newprompt\"\n",
    "trainer_new_prompt = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum_new_prompt[\"train\"],\n",
    "    eval_dataset=tokenized_samsum_new_prompt[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir = \"t5-samsum-newprompt\"\n",
    "trainer_new_prompt = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_samsum_new_prompt[\"train\"],\n",
    "    eval_dataset=tokenized_samsum_new_prompt[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_new_prompt.train()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
